## 1. Follow-up on "Medically Validated Rural Health Images"

**The Question:** "You mentioned you're training your model on 'medically validated rural health images.' Where did you get this dataset, and what does the 'medical validation' process actually involve? Who is validating it?"

**Your Answer:**

- **G (Guide - Set the Stage):** "That's a great question, because sourcing high-quality, ethical medical data is one of the biggest challenges in this field, and it's something we take very seriously."
- **P (Point - The Direct Answer):** "For the Sprint 1 proof-of-concept, our `vision-test` feature uses a general-purpose public dataset, COCO-SSD, to prove the on-device pipeline works. For Sprint 2, our plan is to use a public, anonymized dermatological dataset for initial training."
- **S (Show - Land with Proof):** "The crucial 'medical validation' step is planned for our third sprint, where we'll partner with a research group at a university, like the U of C's medical school. The process involves having licensed physicians review and annotate a subset of our training images to confirm their clinical accuracy for our specific rural triage use case. We can't build a trustworthy model without that expert validation."

**Guidance:** This answer is strong because it's honest about your current state (using a public dataset for a proof-of-concept) while demonstrating a clear, credible, and ethical plan for the future. It shows you understand the complexity and are approaching it responsibly.

---

## 2. Follow-ups on "Combining Text and Image Analysis"

**The Question:** "You said you combine text and images to reduce false positives. How do you know it _reduces_ them? Have you benchmarked it?"

**Your Answer:**

- **G (Guide - Set the Stage):** "That gets to our core hypothesis—that more context leads to better guidance. You're right that we need to prove that claim with data."
- **P (Point - The Direct Answer):** "Our formal benchmark is a key deliverable for Sprint 2. Our plan is to run a standardized set of 100 test cases through two versions of our AI: one text-only, and our multi-modal version."
- **S (Show - Land with Proof):** "We'll then compare the triage recommendations from both models against a 'gold standard' baseline provided by a consulting physician. This will allow us to **quantitatively measure the reduction in false positives** and prove the value of the multi-modal approach."

**Guidance:** You turn a challenge ("Prove it") into an opportunity to showcase your team's scientific rigor. It shows you understand that claims require evidence and you have a clear plan to generate that evidence.

---

**The Follow-up:** "What happens when the text and the image seem to **contradict** each other?"

**Your Answer:**

- **G (Guide - Set the Stage):** "That's a classic edge-case, and our design principle for handling it is to always prioritize the user's self-reported experience."
- **P (Point - The Direct Answer):** "In any case of clear contradiction, our system is designed to default to the data source that indicates a **higher potential severity.**"
- **S (Show - Land with Proof):** "So in your example of a minor scratch photo but the text 'severe bleeding,' the system would absolutely prioritize the text. The user's description and their severity rating are treated as the ground truth, because pain and distress are critical triage indicators that an image alone can't capture."

**Guidance:** This answer provides a clear, logical, and safe rule. It demonstrates responsible system design and a deep understanding of the problem domain (triage).

---

## 3. Follow-ups on "Making Conservative Recommendations"

**The Question:** "You err on the side of caution when the system is 'uncertain.' How do you technically define 'uncertainty'?"

**Your Answer:**

- **G (Guide - Set the Stage):** "That's a great technical question. Translating an abstract concept like 'uncertainty' into code is a key part of building a safe AI."
- **P (Point - The Direct Answer):** "We define uncertainty in two primary ways: For the on-device model, it's any classification that returns a **confidence score below a 75% threshold**. For the Gemini LLM, it's when the model's response includes specific keywords we monitor for, like 'unclear,' 'possibilities are broad,' or 'cannot determine'."
- **S (Show - Land with Proof):** "We arrived at the 75% threshold through testing on our validation set. It gave us the best balance—catching the most ambiguous cases without being overly cautious on clear-cut ones. This is a parameter we'll continue to tune as we gather more data."

**Guidance:** This answer is excellent because it's specific, technical, and demonstrates a thoughtful, data-informed process. It proves you've moved from idea to implementation.

---

**The Follow-up:** "What about the risk of **alarm fatigue**?"

**Your Answer:**

- **G (Guide - Set the Stage):** "Alarm fatigue is a huge issue in digital health. You're right, if we're always crying wolf, users will stop taking the app seriously."
- **P (Point - The Direct Answer):** "Our solution is to provide **graded recommendations** rather than a simple, binary 'go to the doctor / don't go' answer."
- **S (Show - Land with Proof):** "So, instead of just saying 'Seek professional care,' the AI might provide a recommendation with a specific urgency, like: 'This doesn't appear to be an immediate emergency, but you should book a non-urgent appointment with your doctor within the next week.' By providing a **nuanced level of urgency**, we reduce fatigue, build trust, and empower the user to prioritize."

**Guidance:** This answer shows sophisticated product thinking. It demonstrates that you're considering the human side of the interaction and have designed a solution that is both safe and genuinely useful.

---

## 4. Follow-ups on "Monitoring for Algorithmic Bias"

**The Question:** "How are you monitoring for bias across demographics without violating user privacy?"

**Your Answer:**

- **G (Guide - Set the Stage):** "That's the central ethical challenge of building fair AI. You need data to prove fairness, but you must protect that same data."
- **P (Point - The Direct Answer):** "Our approach is to use **voluntary and anonymized** demographic data, which is collected during onboarding and stored completely separately from a user's health profile."
- **S (Show - Land with Proof):** "This allows us to run offline statistical tests on the aggregate data. For example, we can measure the false positive rate across different self-reported skin tones on our entire image dataset **without ever knowing the skin tone of a specific, individual user.** It lets us audit the model for bias at a systemic level, not at the individual level."

**Guidance:** This is a strong, industry-standard answer. It shows you understand the ethical complexity and have a sound technical and privacy-preserving plan to address it.

---

## 5. Follow-ups on "Following HIA and PIPEDA"

**The Question:** "Can you give a specific example of an architectural decision directly driven by a requirement in Alberta's Health Information Act (HIA)?"

**Your Answer:**

- **G (Guide - Set the Stage):** "Absolutely. A core principle of both HIA and Canada's PIPEDA is **data minimization**—only collecting and retaining what is absolutely necessary for the service."
- **P (Point - The Direct Answer):** "This principle directly influenced the design of our `healthEntries` table in our Convex database schema."
- **S (Show - Land with Proof):** "Specifically, while a user can upload a photo for an assessment, **we do not store that raw image file on our servers by default.** The AI processes it, and we only persist the resulting text context. The photo is discarded from the backend unless the user explicitly saves it. That choice—to not retain identifiable health images by default—is a direct architectural implementation of the data minimization principle."

**Guidance:** This answer is powerful because it connects a legal principle directly to a specific, tangible piece of your architecture (`schema.ts`), proving you've internalized the requirements and they've had a real impact on your code.

---

## 6. Follow-ups on "Maintaining Transparency"

**The Question:** "How does your UI/UX design actively prevent misunderstanding of the 'guidance, not diagnosis' disclaimer?"

**Your Answer:**

- **G (Guide - Set the Stage):** "You're right, users often ignore fine print. Our goal was to embed safety and transparency directly into the design of the results screen."
- **P (Point - The Direct Answer):** "We use a design pattern we call 'Suggest, Don't Declare' which is reflected in all of our UI copy and components."
- **S (Show - Land with Proof):** "For example, instead of the AI saying 'You may have X,' the UI presents the information under a heading like **'Possible Factors to Discuss with a Doctor.'** We also use color-coded urgency levels, and every single result screen has a prominent, non-dismissible button that says **'Find a Clinic Nearby.'** The design constantly reinforces that the next step is professional consultation."

**Guidance:** This answer shows that you are a thoughtful product engineer, not just a coder. It proves you're using design as a tool to mitigate risk and ensure user safety.
